from pathlib import Path
from typing import Any

import torch
from torch import Tensor
from torch.nn import BCEWithLogitsLoss
from torch.nn.utils import clip_grad_norm_
from torch.optim import Optimizer
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from src.tabular_sense.components.config import Config
from src.tabular_sense.components.dropout_scheduler import DropoutScheduler
from src.tabular_sense.components.metrics import Metrics
from src.tabular_sense.components.metrics import MultiLabelMetrics
from src.tabular_sense.components.model import Model
from src.tabular_sense.components.resume_strategy import ResumeStrategy
from src.tabular_sense.path import get_models_dir, get_logs_dir


class Trainer:
    """è®­ç»ƒå™¨"""

    model: Model
    device: torch.device
    config: Config
    train_loader: DataLoader
    val_loader: DataLoader
    optimizer: Optimizer
    lr_scheduler: ReduceLROnPlateau
    dp_scheduler: DropoutScheduler

    early_stop_count: int
    early_stop_patience: int
    train_name: str
    start_epoch: int
    epochs: int
    criterion: BCEWithLogitsLoss
    metrics: MultiLabelMetrics
    summary: SummaryWriter
    best_score: float
    best_epoch: int
    checkpoint_dir: Path

    def __init__(
            self,
            model: Model,
            device: torch.device,
            config: Config,
            train_loader: DataLoader,
            val_loader: DataLoader,
            optimizer: Optimizer,
            lr_scheduler: ReduceLROnPlateau,
            dp_scheduler: DropoutScheduler,
            train_name: str,
            early_stop_patience: int = 6,
            epochs: int = 50,
    ):
        """
        :param train_name: è®­ç»ƒåç§°ï¼Œå°†ä½œä¸ºæ—¥å¿—è·¯å¾„ã€æ¨¡å‹ä¿å­˜è·¯å¾„çš„ä¸€éƒ¨åˆ†
        """

        self.model = model
        self.device = device
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.dp_scheduler = dp_scheduler

        self.early_stop_count = 0
        self.early_stop_patience = early_stop_patience
        self.train_name = train_name
        self.start_epoch = 1
        self.epochs = epochs
        self.criterion = BCEWithLogitsLoss()
        self.metrics = MultiLabelMetrics()
        self.summary = SummaryWriter(str(get_logs_dir() / train_name))
        self.best_score = 0
        self.best_epoch = 1
        self.checkpoint_dir = get_models_dir() / f"checkpoint/{train_name}"

        if not self.checkpoint_dir.exists():
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def load_checkpoint(
            self,
            checkpoint_name: str,
            resume_strategy: ResumeStrategy,
            reset_training_state: bool = False,
    ):
        """ä»å­˜æ¡£ç‚¹ä¸­æ¢å¤çŠ¶æ€"""

        print(f"â–¶ å°è¯•åŠ è½½å­˜æ¡£ç‚¹: {checkpoint_name}")
        checkpoint_path = self.checkpoint_dir / f"checkpoint_{checkpoint_name}_best.pt"

        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint file {checkpoint_path} not found")

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint["model_state"])
        print("âœ“ æ¨¡å‹æƒé‡å·²åŠ è½½")

        if resume_strategy == ResumeStrategy.ALL_COMPONENTS:
            self.optimizer.load_state_dict(checkpoint["optimizer_state"])
            self.lr_scheduler.load_state_dict(checkpoint["lr_scheduler_state"])
            self.dp_scheduler.load_state_dict(checkpoint["dp_scheduler_state"])

            print("âœ“ å®Œå…¨æ¢å¤ï¼šä¼˜åŒ–å™¨ + LRè°ƒåº¦å™¨ + ä¸¢å¼ƒç‡è°ƒåº¦å™¨")

        elif resume_strategy == ResumeStrategy.EXCLUDE_OPTIMIZATION:
            self.dp_scheduler.load_state_dict(checkpoint["dp_scheduler_state"])

            lr = checkpoint["optimizer_state"]["param_groups"][0]["lr"]
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

            print(f"âœ“ éƒ¨åˆ†æ¢å¤ï¼šä¸¢å¼ƒç‡è°ƒåº¦å™¨ + checkpointå­¦ä¹ ç‡")
            print(f"âœ— ä¼˜åŒ–å™¨å’ŒLRè°ƒåº¦å™¨ä½¿ç”¨æ–°é…ç½®")

        elif resume_strategy == ResumeStrategy.EXCLUDE_REGULARIZATION:
            self.optimizer.load_state_dict(checkpoint["optimizer_state"])
            self.lr_scheduler.load_state_dict(checkpoint["lr_scheduler_state"])

            self.dp_scheduler.current_dropout = checkpoint["dp_scheduler_state"]["current_dropout"]
            self.dp_scheduler.train_losses = checkpoint["dp_scheduler_state"]["train_losses"]
            self.dp_scheduler.val_losses = checkpoint["dp_scheduler_state"]["val_losses"]

            print(f"âœ“ éƒ¨åˆ†æ¢å¤ï¼šä¼˜åŒ–å™¨ + LRè°ƒåº¦å™¨")
            print(f"âœ— ä¸¢å¼ƒç‡è°ƒåº¦å™¨ä½¿ç”¨æ–°é…ç½®ï¼Œä»…æ¢å¤checkpointçš„dropoutå’Œå†å²loss")

        else:
            lr = checkpoint["optimizer_state"]["param_groups"][0]["lr"]
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

            self.dp_scheduler.current_dropout = checkpoint["dp_scheduler_state"]["current_dropout"]
            self.dp_scheduler.train_losses = checkpoint["dp_scheduler_state"]["train_losses"]
            self.dp_scheduler.val_losses = checkpoint["dp_scheduler_state"]["val_losses"]

            print(f"âœ— ä¼˜åŒ–å™¨ã€LRè°ƒåº¦å™¨ä½¿ç”¨æ–°é…ç½®ï¼Œä»…æ¢å¤checkpointçš„å­¦ä¹ ç‡å’Œéƒ¨åˆ†dropoutçŠ¶æ€")

        if not reset_training_state:
            self.best_score = checkpoint["best_score"]
            self.best_epoch = checkpoint["best_epoch"]
            self.early_stop_count = checkpoint["early_stop_count"]
            self.start_epoch = checkpoint["start_epoch"]

    def train(self):
        print("=" * 60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ")
        print(f"    Epochs: {self.start_epoch} -> {self.epochs}")
        print(f"    æ¨¡å‹æ¶æ„: {self.config}")
        print(f"    å‚æ•°è§„æ¨¡: {self.model.param_num}")
        print(f"    å½“å‰å­¦ä¹ ç‡: {self.lr_scheduler.get_last_lr()[0]:.2e}")
        print(f"    å½“å‰Dropout: {self.dp_scheduler.current_dropout}")
        print(f"    æœ€ä½³åˆ†æ•°: {self.best_score}")
        print("=" * 60)

        for epoch in range(self.start_epoch, self.epochs + 1):
            train_loss = self.train_epoch(epoch)
            val_loss, metrics = self.validate_epoch(epoch)

            print(f"â­ï¸ Epoch {epoch}/{self.epochs}")
            print(f"    Train loss: {train_loss:.8f}")
            print(f"    Val loss: {val_loss:.8f}")
            print(f"    Score: {metrics.score:.8f}")
            print(f"    F1: {metrics.f1:.8f}")
            print(f"    LR: {self.lr_scheduler.get_last_lr()[0]:.2e}")
            print(f"    DP: {self.dp_scheduler.current_dropout}")

            old_lr = self.lr_scheduler.get_last_lr()[0]
            self.lr_scheduler.step(val_loss)
            new_lr = self.lr_scheduler.get_last_lr()[0]

            if old_lr != new_lr:
                print(f"ğŸ”„ å­¦ä¹ ç‡è°ƒæ•´ ({epoch}): {old_lr:.2e} -> {new_lr:.2e}")
                self.summary.add_text("Hyperparams", f"ğŸ”„ å­¦ä¹ ç‡è°ƒæ•´ ({epoch}): {old_lr:.2e} -> {new_lr:.2e}", epoch)

            old_dp = self.dp_scheduler.current_dropout
            new_dp = self.dp_scheduler.step(train_loss, val_loss)

            if old_dp != new_dp:
                self.early_stop_count = 0

                print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆè¶‹åŠ¿ ({epoch})")
                print(f"ğŸ”„ Epoch {epoch}: Dropout {old_dp:.3f} â†’ {new_dp:.3f}, æ—©åœè®¡æ•°é‡ç½®")

                self.summary.add_text(
                    "Hyperparams",
                    f"âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆè¶‹åŠ¿ ({epoch})\nğŸ”„ Epoch {epoch}: Dropout {old_dp:.3f} â†’ {new_dp:.3f}, æ—©åœè®¡æ•°é‡ç½®",
                    epoch,
                )

            self.summary.add_scalars("Training/Loss", {
                "train": train_loss,
                "val": val_loss,
            }, epoch)
            self.summary.add_scalar("Training/Score", metrics.score, epoch)
            self.summary.add_scalars("Metrics", {
                "Precision": metrics.precision,
                "Recall": metrics.recall,
                "F1": metrics.f1,
            }, epoch)
            self.summary.add_scalar("Metrics/Hamming Loss", metrics.hamming_loss, epoch)
            self.summary.add_scalar("Metrics/EM", metrics.em, epoch)
            self.summary.add_scalar("Hyperparams/LR", self.lr_scheduler.get_last_lr()[0], epoch)
            self.summary.add_scalar("Hyperparams/DP", self.dp_scheduler.current_dropout, epoch)

            if self._is_best(metrics, epoch):
                print(f"âœ¨ æ–°çš„æœ€ä½³æ¨¡å‹ (Epoch {self.best_epoch}, Score={metrics.score:.8f})")
                self.summary.add_text(
                    "BestModel",
                    f"âœ¨ æ–°çš„æœ€ä½³æ¨¡å‹ (Epoch {self.best_epoch}, Score={metrics.score:.8f})",
                    epoch,
                )
                torch.save(
                    {
                        "model_state": self.model.state_dict(),
                        "optimizer_state": self.optimizer.state_dict(),
                        "lr_scheduler_state": self.lr_scheduler.state_dict(),
                        "dp_scheduler_state": self.dp_scheduler.state_dict(),
                        "best_score": self.best_score,
                        "best_epoch": self.best_epoch,
                        "early_stop_count": self.early_stop_count,
                        "start_epoch": epoch + 1,
                    },
                    self.checkpoint_dir / f"checkpoint_{self.train_name}_best.pt",
                )

            self.summary.add_scalars("Early Stopping", {
                "early_stop_count": self.early_stop_count,
                "early_stop_patience": self.early_stop_patience,
            }, epoch)

            if self.early_stop_count >= self.early_stop_patience:
                print(f"ğŸš¨ æ—©åœè§¦å‘: è¿ç»­ {self.early_stop_patience} ä¸ªepochæ— æå‡")
                print(f"    æœ€ä½³æ¨¡å‹: Epoch {self.best_epoch}, Score={self.best_score:.8f}")
                self.summary.add_text(
                    "EarlyStop",
                    f"ğŸš¨ æ—©åœè§¦å‘: è¿ç»­ {self.early_stop_patience} ä¸ªepochæ— æå‡\n    æœ€ä½³æ¨¡å‹: Epoch {self.best_epoch}, Score={self.best_score:.8f}",
                    epoch,
                )
                break

        self.summary.close()
        print("=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆ")
        print(f"    æœ€ä½³åˆ†æ•°: {self.best_score:.8f} (Epoch {self.best_epoch})")

    def train_epoch(self, epoch: int) -> float:
        self.model.train()
        total_loss = 0.0
        progress: tqdm[dict[str, Any]] = tqdm(self.train_loader, f"Epoch {epoch}/{self.epochs} [Train]")

        for idx, batch in enumerate(progress):
            logits, loss, labels = self._predict(batch)
            total_loss += loss.item()

            self.optimizer.zero_grad()
            loss.backward()

            if idx % 100 == 0:
                self._record_gradients(epoch)

            clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            self.optimizer.step()

            progress.set_postfix({
                "loss": f"{loss.item():.8f}",
                "lr": f"{self.lr_scheduler.get_last_lr()[0]:.2e}",
                "dp": f"{self.dp_scheduler.current_dropout}",
            })

        avg_loss = total_loss / len(self.train_loader)
        progress.set_postfix({
            "loss": f"{avg_loss:.8f}",
            "lr": f"{self.lr_scheduler.get_last_lr()[0]:.2e}",
            "dp": f"{self.dp_scheduler.current_dropout}",
        })
        return avg_loss

    @torch.no_grad()
    def validate_epoch(self, epoch: int) -> tuple[float, Metrics]:
        self.model.eval()
        total_loss = 0.0
        all_predictions: list[Tensor] = []
        all_labels: list[Tensor] = []
        progress: tqdm[dict[str, Any]] = tqdm(self.val_loader, f"Epoch {epoch}/{self.epochs} [Val]")

        for batch in progress:
            logits, loss, labels = self._predict(batch)
            total_loss += loss.item()
            probabilities = torch.sigmoid(logits)
            prediction = (probabilities > 0.5).float()

            all_predictions.append(prediction)
            all_labels.append(labels)

            progress.set_postfix({
                "loss": f"{loss.item():.8f}",
                "lr": f"{self.lr_scheduler.get_last_lr()[0]:.2e}",
                "dp": f"{self.dp_scheduler.current_dropout}",
            })

        avg_loss = total_loss / len(self.val_loader)
        progress.set_postfix({
            "loss": f"{avg_loss:.8f}",
            "lr": f"{self.lr_scheduler.get_last_lr()[0]:.2e}",
            "dp": f"{self.dp_scheduler.current_dropout}",
        })
        return avg_loss, self.metrics(all_predictions, all_labels)

    def _predict(self, batch: dict[str, Any]) -> tuple[Tensor, Tensor, Tensor]:
        """
        æ¨¡å‹é¢„æµ‹
        
        :return tuple[logits, loss, labels]
        """

        input_ids = batch["input_ids"].to(self.config.device)
        attention_masks = batch["attention_masks"].to(self.config.device)
        labels = batch["labels"].to(self.config.device)

        logits = self.model(input_ids, attention_masks)
        loss = self.criterion(logits, labels)
        return logits, loss, labels

    def _is_best(self, metrics: Metrics, epoch: int) -> bool:
        """åˆ¤æ–­å¹¶æ›´æ–°æœ€ä½³è®°å½•å’Œæ—©åœè®¡æ•°"""

        improvement = metrics.score - self.best_score
        if improvement > self.config.min_delta:
            if self.early_stop_count > 0:
                print(f"âœ”ï¸ æ—©åœè®¡æ•°é‡è®¾ ({epoch})")
                self.summary.add_text("EarlyStop", f"âœ”ï¸ æ—©åœè®¡æ•°é‡è®¾ ({epoch})", epoch)

            self.best_score = metrics.score
            self.best_epoch = epoch
            self.early_stop_count = 0

            return True
        else:
            self.early_stop_count += 1

            if self.early_stop_count < self.early_stop_patience // 2:
                symbol = "â³"
            elif self.early_stop_count < self.early_stop_patience * 0.8:
                symbol = "âš ï¸"
            else:
                symbol = "ğŸš¨"

            print(f"{symbol} æ¥è¿‘æ—©åœé˜ˆå€¼ ({self.early_stop_count}/{self.early_stop_patience})")
            self.summary.add_text(
                "EarlyStop",
                f"{symbol} æ¥è¿‘æ—©åœé˜ˆå€¼ ({self.early_stop_count}/{self.early_stop_patience})",
                epoch,
            )
            return False

    def _record_gradients(self, epoch: int):
        """è®°å½•å…³é”®ç»„ä»¶çš„æ¢¯åº¦èŒƒæ•°åˆ°TensorBoard"""

        def get_component_group(component: str) -> str:
            """æ ¹æ®å‚æ•°åç§°ç¡®å®šæ‰€å±ç»„ä»¶åˆ†ç»„"""
            if component == "cls_parameter":
                return "cls_parameter"
            elif component == "embedding.weight":
                return "embedding"
            elif component.startswith("encoder.layers.0."):
                return "encoder_first"
            elif component.startswith(f"encoder.layers.{self.config.n_encoder_layers // 2}."):
                return "encoder_mid"
            elif component.startswith(f"encoder.layers.{self.config.n_encoder_layers - 1}."):
                return "encoder_last"
            else:
                return "classifier"

        # åˆå§‹åŒ–æ¢¯åº¦åˆ†ç»„
        grad_groups = {}

        # éå†æ‰€æœ‰å‚æ•°ï¼Œæ”¶é›†æ¢¯åº¦èŒƒæ•°
        for name, param in self.model.named_parameters():
            if param.grad is None:
                continue

            group = get_component_group(name)
            if group is None:
                continue

            grad_norm = param.grad.norm().item()
            grad_groups.setdefault(group, []).append(grad_norm)

        # å†™å…¥TensorBoardï¼ˆæ¯ç»„å–å¹³å‡ï¼‰
        for group, norms in grad_groups.items():
            if norms:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                avg_norm = sum(norms) / len(norms)
                self.summary.add_scalar(f"GradNorm/{group}", avg_norm, epoch)

        # è®°å½•å…¨å±€æ¢¯åº¦èŒƒæ•°ï¼ˆæ‰€æœ‰å‚æ•°ï¼‰
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.norm().item()
                total_norm += param_norm ** 2
        total_norm = total_norm ** 0.5
        self.summary.add_scalar("GradNorm/global", total_norm, epoch)
