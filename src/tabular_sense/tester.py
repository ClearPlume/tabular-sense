import torch
from torch import Tensor
from torch.nn import BCEWithLogitsLoss
from torch.utils.data import DataLoader
from tqdm import tqdm

from src.tabular_sense.components.collate import collate_fn
from src.tabular_sense.components.config import Config
from src.tabular_sense.components.dataset import ColumnDataset, BatchedColumnSample
from src.tabular_sense.components.logger import setup_logger
from src.tabular_sense.components.metrics import MultiLabelMetrics
from src.tabular_sense.components.model import Model
from src.tabular_sense.components.sampler import LengthGroupSampler
from src.tabular_sense.components.tokenizer import Tokenizer
from src.tabular_sense.core.enum_util import ColumnType
from src.tabular_sense.path import get_data_dir, get_models_dir


def run_test(
        model: Model,
        test_loader: DataLoader[ColumnDataset],
        criterion: BCEWithLogitsLoss,
) -> tuple[float, list[Tensor], list[Tensor]]:
    """æ ¸å¿ƒæµ‹è¯•é€»è¾‘ï¼šè¿è¡Œæ¨¡åž‹æŽ¨ç†å¹¶æ”¶é›†ç»“æžœ

    Args:
        model: å¾…æµ‹è¯•çš„æ¨¡åž‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°

    Returns:
        (å¹³å‡æŸå¤±, é¢„æµ‹ç»“æžœåˆ—è¡¨, çœŸå®žæ ‡ç­¾åˆ—è¡¨)
    """
    device = next(model.parameters()).device
    total_loss = 0
    all_predictions: list[Tensor] = []
    all_labels: list[Tensor] = []

    progress: tqdm[BatchedColumnSample] = tqdm(test_loader, "[Test]")

    for batch in progress:
        input_ids = batch.input_ids.to(device)
        attention_masks = batch.attention_masks.to(device)
        labels = batch.labels.to(device)

        with torch.no_grad():
            logits = model(input_ids, attention_masks)

        total_loss += criterion(logits, labels).item()
        probabilities = torch.sigmoid(logits)
        prediction = (probabilities > 0.5).float()
        all_predictions.append(prediction)
        all_labels.append(labels)

    avg_loss = total_loss / len(test_loader)
    return avg_loss, all_predictions, all_labels


def batch_test(name: str):
    """æ‰¹é‡æµ‹è¯•æ¨¡å¼ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡åž‹"""
    logger = setup_logger(name, "test")
    metrics = MultiLabelMetrics()
    criterion = BCEWithLogitsLoss()

    model, test_loader, config = initialize(name)

    logger.info("=" * 60)
    logger.info("ðŸ§ª å¼€å§‹æµ‹è¯•")
    logger.info(f"    Checkpoint: {name}")
    logger.info(f"    æ¨¡åž‹æž¶æž„: {config.d_model}dÃ—{config.n_head}hÃ—{config.n_encoder_layers}L")
    logger.info(f"    å‚æ•°è§„æ¨¡: {model.param_num}")
    logger.info(f"    æµ‹è¯•æ ·æœ¬: {len(test_loader) * config.batch_size}")
    logger.info(f"    æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    logger.info("=" * 60)

    avg_loss, all_predictions, all_labels = run_test(model, test_loader, criterion)
    result = metrics(all_predictions, all_labels)

    logger.info("=" * 60)
    logger.info("ðŸ“Š æµ‹è¯•ç»“æžœ")
    logger.info(f"    Loss: {avg_loss:.8f}")
    logger.info(f"    Score: {result.score:.8f}")
    logger.info(f"    F1: {result.f1:.8f}")
    logger.info(f"    Precision: {result.precision:.8f}")
    logger.info(f"    Recall: {result.recall:.8f}")
    logger.info(f"    Hamming Loss: {result.hamming_loss:.8f}")
    logger.info(f"    EM: {result.em:.8f}")
    logger.info("=" * 60)


def interactive_test(name: str):
    """äº¤äº’æµ‹è¯•æ¨¡å¼ï¼šæ”¯æŒå•æ ·æœ¬å®žæ—¶é¢„æµ‹"""
    config = Config.final()
    tokenizer = Tokenizer()

    model = Model(tokenizer.vocab_size, config)
    model.load(name)
    model.eval()

    device = next(model.parameters()).device

    print("=" * 60)
    print("ðŸŽ¯ äº¤äº’æµ‹è¯•æ¨¡å¼")
    print(f"    Checkpoint: {name}")
    print(f"    æ¨¡åž‹æž¶æž„: {config.d_model}dÃ—{config.n_head}hÃ—{config.n_encoder_layers}L")
    print(f"    å‚æ•°è§„æ¨¡: {model.param_num}")
    print("=" * 60)
    print("è¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹ (è¾“å…¥ 'quit' é€€å‡º):")

    while True:
        data = input("\næ•°æ®> ").strip()

        if data.lower() in ["quit", "exit", "q"]:
            print("é€€å‡ºäº¤äº’æµ‹è¯•")
            break

        if not data:
            continue

        # Tokenize
        tokens = tokenizer.encode(data)[:config.max_len]
        input_ids = torch.tensor(tokens).unsqueeze(0).to(device)
        attention_mask = torch.ones(len(tokens)).unsqueeze(0).to(device)

        # æŽ¨ç†
        with torch.no_grad():
            logits = model(input_ids, attention_mask)

        probabilities = torch.sigmoid(logits)
        column_types = list(ColumnType)
        type_probabilities = {column_type.name: float(probabilities[0][idx]) for idx, column_type in
                              enumerate(column_types)}
        result = dict(sorted(type_probabilities.items(), key=lambda x: x[1], reverse=True)[:5])

        # è¾“å‡ºç»“æžœ
        print(result)


def initialize(name: str) -> tuple[Model, DataLoader[ColumnDataset], Config]:
    config = Config.final()
    tokenizer = Tokenizer()

    model = Model(tokenizer.vocab_size, config)
    model.load(name)
    model.eval()

    dataset = ColumnDataset(data_dir / "samples/samples.txt", tokenizer, config)
    _, _, test_dataset = dataset.split()
    test_loader = DataLoader(
        dataset=test_dataset,
        collate_fn=collate_fn,
        batch_sampler=LengthGroupSampler("test", test_dataset, config.batch_size, True),
        num_workers=4,
        pin_memory=True,
    )

    return model, test_loader, config


def main():
    """ä¸»å…¥å£ï¼šæ”¯æŒå¤šç§æµ‹è¯•æ¨¡å¼"""
    import sys

    mode = sys.argv[1] if len(sys.argv) > 1 else "batch"
    name = sys.argv[2] if len(sys.argv) > 2 else "2025-10-30"

    if mode == "interactive":
        interactive_test(name)
    else:
        batch_test(name)


if __name__ == '__main__':
    data_dir = get_data_dir()
    model_dir = get_models_dir()
    main()
